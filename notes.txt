Hypothesis to test: if ChangeConfigurations always arrive at all nodes at the
very beginning, will the same bugs appear?

Another: does running Shrink() multiple times change things?

TODO(cs): should always be UniqueMsgEvents, not MsgEvents in trace.events.
Assert that!

== Minimizing internals ==

DPOR that just tries to remove (not add or reorder) internal events. Might be
able to do this statically based on DepGraph!

== Improved DDMin ==

DDMin that splits along DAG (DepGraph) lines...

IDEA: try first pruning external events that have long happens-before
chains leading up to bug.

== Edit Distance ==

Online version using a prefix trie

== Hacks ==

Possibly find a hackier version of CheckpointRequests that doesn't require us
to send messages.
----------------
TODO(cs): graph a breakdown of (i) deliveries pruned by provenance, (ii)
deliveries pruned by DDMin, (iii) deliveries pruned by internal minimization.
TODO(cs): clean up how we deserialize MCSes. In general, decouple
deserialization from the RunnerUtil execution functions.
TODO(cs): provenance should consider TimerDeliveries
TODO(cs): another degree of freedom we should emperically validate: how we
react to absent events.
TODO(cs): currently only DPORwHeuristics makes use of provenance optimization.
TODO(cs): After ~35 validation attempts, the system sometimes hangs. We probably have a
race condition somewhere.
TODO(cs): maybe no need to store DepGraph. It's already there in event_orchestrator.events!
TODO(cs): PeekScheduler and RandomScheduler should call
stats.increment_replays() more than once, even if they don't ever find the bug...
TODO(cs): check that clear()'ing of Instrumenter().random() doesn't screw up
checkpoints.
TODO(cs): deal with sts2-bugs.txt
TODO(cs): the no pending timers for alive actors issue might affect other schedulers
          that don't invoke handle_quiescence.
TODO(cs): right-to-left-removal
TODO(cs): if any repeating timer t went off k times in \tau^0, we should make sure it only goes
off k times in the executions we care about (i.e., we don't have to worry
about it going off infinitely, etc.)
TODO(cs): understand why my optimization of pruning out Sends to non-existent
actors wasn't correct
TODO(cs): timers break eventTrace.filterSends
TODO(cs): CheckpointRequests/Reply should really be handled by Instrumenter()
-----------------
TODO(cs): do we handle context.system.eventStream.publish properly? It's
          apparently only a local facility, not a cluster facility.
TODO(cs): raise exception if any of the other scheduler.schedule methods are
          called.
TODO(cs): should we be testing the cluster versions of the actors?
TODO(cs): double check: does akka-raft write anything to disk? Sort of seems
          like it doesn't support crash-recovery.. In fact, it doesn't make
          use of any of akka's persistence module...

----------- Random notes -----------

// Potentially useful:
http://stackoverflow.com/questions/17183308/scala-reflection-of-nested-list

// Andi's aspectj:
https://github.com/andiwundsam/floodlight-sync-proto/commit/d1e75dfb51b5d2d0847e5a4d2a3bc756df423bb0

// akka-raft Only uses:
import scala.concurrent.forkjoin.ThreadLocalRandom

----------- Timer notes ------------
Looks there are two uses of timers here:
  - cluster/ClusterRaftActor.scala uses schedule.scheduleOnce(), but I don't
    think it's actually crucial to the protocol. Looks like it's just used for
    cluster membership discovery.

    context.system.scheduler.scheduleOnce(identifyTimeout, self, onIdentityTimeout)

  - LoggingFSM.setTimer (part of the Akka) is used for heartbeats, etc.
    repeat is set to both true (for Leader) and false (for RaftActor)

  setTimer implementation:

  private[akka] case class Timer(name: String, msg: Any, repeat: Boolean, generation: Int)(context: ActorContext)
    extends NoSerializationVerificationNeeded {
    private var ref: Option[Cancellable] = _
    private val scheduler = context.system.scheduler
    private implicit val executionContext = context.dispatcher

    def schedule(actor: ActorRef, timeout: FiniteDuration): Unit =
      ref = Some(
        if (repeat) scheduler.schedule(timeout, timeout, actor, this)
        else scheduler.scheduleOnce(timeout, actor, this))


  So, the two scheduler interfaces used are:
    - scheduleOnce (delay: Duration, receiver: ActorRef, message: Any): Cancellable
    - schedule (initialDelay: Duration, frequency: Duration, receiver:ActorRef, message: Any): Cancellable
