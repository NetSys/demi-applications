When implementing incremental DDMin, should really keep backTracks around, or
restore them somehow for each subsequence.

DPOR that tracks the smallest failing execution it has seen so far.

DDMin that iteratively feeds larger and larger edit-distances to DPOR.

DDMin that splits along DAG (DepGraph) lines...

Possibly find a hackier version of CheckpointRequests that doesn't require us
to send messages.

DPOR that just tries to remove (not add or reorder) internal events. Might be
able to do this statically based on DepGraph!

IDEA: try first pruning external events that have long happens-before
chains leading up to bug.

Even with incremental edit-istance DDMin, there might be redundant external
event sequences that we feed to it? Then again, DPOR might just stop
immediately once it realizes it has already explored all those schedules.

Edit distance metric that takes into account causal chains. Take union of
longest common subsequences for each causal chain. Basically, don't want to
penalize commutative reorderings.
----------------
TODO(cs): another degree of freedom we should emperically validate: how we
react to absent events.
TODO(cs): currently only DPORwHeuristics makes use of provenance optimization.
TODO(cs): Write verify_mcs for DPOR. Not clear what initialTrace to give it, since DDMin
might not actually replay the execution -- it might be pieced together.
TODO(cs): After ~35 validation attempts, the system sometimes hangs. We probably have a
race condition somewhere.
TODO(cs): maybe no need to store DepGraph. It's already there in
event_orchestrator.events!
TODO(cs): PeekScheduler and RandomScheduler should call
stats.increment_replays() more than once, even if they don't ever find the bug...
TODO(cs): check that clear()'ing of Instrumenter().random() doesn't screw up
checkpoints.
TODO(cs): deal with sts2-bugs.txt
TODO(cs): the no pending timers for alive actors issue might affect other schedulers
          that don't invoke handle_quiescence.
TODO(cs): right-to-left-removal
TODO(cs): save smallest trace recorded so far. Not necessarily MCS!
TODO(cs): if any repeating timer t went off k times in \tau^0, we should make sure it only goes
off k times in the executions we care about (i.e., we don't have to worry
about it going off infinitely, etc.)
TODO(cs): understand why my optimization of pruning out Sends to non-existent
actors wasn't correct
TODO(cs): don't actually need to generate DepGraph in RandomScheduler.scala.
Could infer it Panda's way from the EventTrace.
TODO(cs): seems like all these handleTick's during fuzzing are unnecessary...
In fact, why are we relying on the akka scheduler at all? Why not just enqueue
timers as soon as they're delivered?
TODO(cs): timers break eventTrace.filterSends
-----------------
TODO(cs): do we handle context.system.eventStream.publish properly? It's
          apparently only a local facility, not a cluster facility.
TODO(cs): raise exception if any of the other scheduler.schedule methods are
          called.
TODO(cs): should we be testing the cluster versions of the actors?
TODO(cs): double check: does akka-raft write anything to disk? Sort of seems
          like it doesn't support crash-recovery.. In fact, it doesn't make
          use of any of akka's persistence module...

----------- Random notes -----------

// Potentially useful:
http://stackoverflow.com/questions/17183308/scala-reflection-of-nested-list

// Andi's aspectj:
https://github.com/andiwundsam/floodlight-sync-proto/commit/d1e75dfb51b5d2d0847e5a4d2a3bc756df423bb0

// akka-raft Only uses:
import scala.concurrent.forkjoin.ThreadLocalRandom

----------- Timer notes ------------
Looks there are two uses of timers here:
  - cluster/ClusterRaftActor.scala uses schedule.scheduleOnce(), but I don't
    think it's actually crucial to the protocol. Looks like it's just used for
    cluster membership discovery.

    context.system.scheduler.scheduleOnce(identifyTimeout, self, onIdentityTimeout)

  - LoggingFSM.setTimer (part of the Akka) is used for heartbeats, etc.
    repeat is set to both true (for Leader) and false (for RaftActor)

  setTimer implementation:

  private[akka] case class Timer(name: String, msg: Any, repeat: Boolean, generation: Int)(context: ActorContext)
    extends NoSerializationVerificationNeeded {
    private var ref: Option[Cancellable] = _
    private val scheduler = context.system.scheduler
    private implicit val executionContext = context.dispatcher

    def schedule(actor: ActorRef, timeout: FiniteDuration): Unit =
      ref = Some(
        if (repeat) scheduler.schedule(timeout, timeout, actor, this)
        else scheduler.scheduleOnce(timeout, actor, this))


  So, the two scheduler interfaces used are:
    - scheduleOnce (delay: Duration, receiver: ActorRef, message: Any): Cancellable
    - schedule (initialDelay: Duration, frequency: Duration, receiver:ActorRef, message: Any): Cancellable
