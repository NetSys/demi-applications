Runner: deploy/SparkSubmit.scala
sched.start(): util/AkkaUtils.scala

Gotchas:

Sometimes need to reboot, due to hostname issues

More than one thread during initialization? Is it only LocalBackend's
ThreadPoolService that creates new threads?

Call stack:
  - SparkContext.parallelize is the starting point.
    -> taskScheduler = TaskSchedulerImpl.initialize(LocalBackend)
    -> new DAGScheduler
    -> taskScheduler.start()
    -> dagScheduler.runJob

Only one ActorSystem, luckily. Created here:
[error]         at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:48)
[error]         at org.apache.spark.SparkEnv$.create(SparkEnv.scala:153)
[error]         at org.apache.spark.SparkContext.<init>(SparkContext.scala:202)
[error]         at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:28)

Actors observed in the system:
  MapOutputTracker
  BlockManagerMaster
  BlockManagerActor1
System has created a new actor: $a [DAGSchedulerEventProcessActor]
System has created a new actor: LocalBackendActor

// Should block out:
//  - akkaprotocolmanager.tcp0
//  - remote-watcher

Actors in the system:
 -  DAGSchedulerActorSupervisor?
 -  DAGSchedulerEventProcessActor?
 -  LocalActor (LocalBackend)
 -  BlockManager(Master|Slave)Actor.scala?
 -  Master
 -  Worker?

TODO(cs):
 - Some dependenance on time: "Checking for hosts with no recent heart beats in BlockManagerMaster."

Relevant:
  SparkEnv.get.actorSystem
  ExecutorExitCode.scala. Used to communicate failures to the master.
  scheduler/local/LocalBackend.scala: the actor (and actor system)

Changes to Spark, for rollback:
 - AkkaUtils.scala => use Instrumenter().actorSystem
 - Symlink to aspectj
 - actorBlocked() before each Await call
 - schedulers/JobWaiter.scala -> endUnignorableEvents
 - executor/Executor.scala
 - util/IdGenerator.scala -> return 1
 [ Alternate to below: use class tags? ]
 - storage/BlockManagerMessages -> non-private
 - scheduler/DAGSchedulerEvents -> non-private
 - scheduler/local/LocalBackend -> non-private
